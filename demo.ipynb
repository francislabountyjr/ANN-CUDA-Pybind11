{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ann_cuda as nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "batch_size = 10\n",
    "learning_rate = 0.02\n",
    "epochs = 1000\n",
    "load = True\n",
    "save = True\n",
    "directory = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset from sklearn\n",
    "X, Y = (load_digits()['data'], load_digits()['target'])\n",
    "X = X.reshape(-1, 1, 8, 8).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode\n",
    "Y_enc = np.zeros((Y.size, 10, 1, 1), dtype=np.float32)\n",
    "for i in range(Y.size):\n",
    "    for x in range(10):\n",
    "        if Y[i] == x:\n",
    "            Y_enc[i,x] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in to batches\n",
    "Y_enc2 = Y_enc.copy()\n",
    "X_batched = np.array_split(X, X.shape[0]/batch_size, axis=0)\n",
    "Y_batched = np.array_split(Y_enc, X.shape[0]/batch_size, axis=0)\n",
    "Y_batched2 =np.array_split(Y_enc2, X.shape[0]/batch_size, axis=0)\n",
    "for i in range(len(X_batched)):\n",
    "    if X_batched[i].shape[0] != batch_size:\n",
    "        X_batched[i] = X_batched[i][:batch_size, :, :, :]\n",
    "        Y_batched[i] = Y_batched[i][:batch_size, :, :, :]\n",
    "        Y_batched2[i] = Y_batched2[i][:batch_size, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading Weights...\n",
      "\n",
      "...Model Configuration...\n",
      "\n",
      "CUDA: input1 \n",
      "\n",
      "CUDA: conv1 \n",
      "\n",
      "CUDA: relu1 \n",
      "\n",
      "CUDA: pool1 \n",
      "\n",
      "CUDA: dense_1 \n",
      "\n",
      "CUDA: relu2 \n",
      "\n",
      "CUDA: dense2 \n",
      "\n",
      "CUDA: softmax1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize network\n",
    "net = nn.Network()\n",
    "\n",
    "# set up network architecture\n",
    "input = net + nn.Input(\"input1\")([1, 8, 8])\n",
    "x = net + nn.Conv2D(\"conv1\", 20, 5, 1, 0, 1)(input)\n",
    "x = net + nn.Activation(\"relu1\", nn.CUDNN_ACTIVATION_RELU, 0)(x)\n",
    "x = net + nn.Pooling(\"pool1\", 2, 0, 2, nn.CUDNN_POOLING_MAX)(x)\n",
    "x = net + nn.Dense(\"dense_1\", 200)(x)\n",
    "x = net + nn.Activation(\"relu2\", nn.CUDNN_ACTIVATION_RELU, 0)(x)\n",
    "x = net + nn.Dense(\"dense2\", 10)(x)\n",
    "out = net + nn.Softmax(\"softmax1\")(x)\n",
    "\n",
    "# load pretrained model weights from specified directory\n",
    "if (load):\n",
    "    net.load(directory)\n",
    "\n",
    "net.cuda() # move to device, topologically sort layer graph to avoid dependency issues, and then find output layers\n",
    "net.train() # set network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[input1\tShape: { 1, 8, 8 },\n",
       " conv1\tOutput Channels: 20,\n",
       " relu1\tActivation Mode: 1,\n",
       " pool1\tPooling Mode: 0,\n",
       " dense_1\tOutput Size: { 200 },\n",
       " relu2\tActivation Mode: 1,\n",
       " dense2\tOutput Size: { 10 },\n",
       " softmax1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view topological sorting of layers\n",
    "net.layers_topological()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[softmax1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output layers\n",
    "net.output_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert batched numpy arrays to Tensor instances and then move to device\n",
    "for i, zipped in enumerate(zip(X_batched, Y_batched, Y_batched2)):\n",
    "    X_batched[i] = nn.Tensor(zipped[0])\n",
    "    X_batched[i].to(nn.cuda)\n",
    "    Y_batched[i] = nn.Tensor(zipped[1])\n",
    "    Y_batched[i].to(nn.cuda)\n",
    "    Y_batched2[i] = nn.Tensor(zipped[2])\n",
    "    Y_batched2[i].to(nn.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..loaded  conv1  pretrained weights and biases..\n",
      "\n",
      "..loaded  dense_1  pretrained weights and biases..\n",
      "\n",
      "..loaded  dense2  pretrained weights and biases..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch, labels in zip(X_batched[30:], Y_batched[30:]):\n",
    "        # tensor = nn.Tensor(batch)\n",
    "        # tensor.to(nn.cuda)\n",
    "        # target = nn.Tensor(labels)\n",
    "        # target.to(nn.cuda)\n",
    "        \n",
    "        net.forward([batch])\n",
    "        net.backward([labels])\n",
    "        net.update(learning_rate)\n",
    "    #print(f\"Epoch {epoch}: {net.get_accuracy([target])[0] * 100 / batch_size}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1: 0.0, Accuracy1: 94.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "# test network\n",
    "net.test()\n",
    "tp_count = [0, 0]\n",
    "step = 0\n",
    "for batch, labels in zip(X_batched[:30], Y_batched[:30]):\n",
    "\tnet.forward([batch])\n",
    "\ttp_count[0] += net.get_accuracy([labels])[0]\n",
    "\n",
    "\t# fetch next data\n",
    "\tstep += 1\n",
    "loss = net.loss([labels])\n",
    "accuracy1 = 100 * tp_count[0] / step / batch_size\n",
    "\n",
    "print(f\"Loss1: {loss[0]}, Accuracy1: {accuracy1}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Storing Weights...\n",
      "\n",
      "..saving input1 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving conv1 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving relu1 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving pool1 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving dense_1 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving relu2 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving dense2 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n",
      "..saving softmax1 weights and biases..\n",
      "\n",
      "..done..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save network weights to specified directory\n",
    "if (save):\n",
    "    net.save(directory)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84010b238558be5cf744c29544b9cd29e58821031d0f6ffab195f6e744c6e048"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
